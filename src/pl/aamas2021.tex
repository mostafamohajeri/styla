%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for AAMAS-2022 (based on sample-sigconf.tex)
%%% Prepared by Ana L. C. Bazzan and Lucas N. Alegre, with the contribution of the AAMAS-2022 Program Chairs. Thanks to Natasha Alechina. (version 2022-07-08)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass command.
%%% Use the first variant below for the final paper.
%%% Use the second variant below for submission.

%\documentclass[sigconf]{aamas} 
\documentclass[sigconf,anonymous]{aamas} 

%%% Load required packages here (note that many are included already).

\usepackage{balance} % for balancing columns on the final page
\usepackage{listings}
\usepackage{minted}
% \usemintedstyle{vs}
%END SYNTAX HIGHLIGHT
\newmintinline[asc]{prolog}{fontsize=\small}
\usepackage{color}
\newcommand{\Gio}[1]{\textcolor{red}{#1}}
\newcommand{\Mos}[1]{\textcolor{blue}{#1}}
\newtheorem{definition}{Definition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% AAMAS-2022 copyright block (do not change!)

\setcopyright{ifaamas}
\acmConference[AAMAS '22]{Proc.\@ of the 21st International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2022)}{May 9--13, 2022}
{Auckland, New Zealand}{P.~Faliszewski, V.~Mascardi, C.~Pelachaud,
M.E.~Taylor (eds.)}
\copyrightyear{2022}
\acmYear{2022}
\acmDOI{}
\acmPrice{}
\acmISBN{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Use this command to specify your EasyChair submission number.
%%% In anonymous mode, it will be printed on the first page.

\acmSubmissionID{???}

%%% Use this command to specify the title of your paper.

\title[AAMAS-2022 Formatting Instructions]{Formatting Instructions for the 21st International Conference on Autonomous Agents and Multiagent Systems}

%%% Provide names, affiliations, and email addresses for all authors.

% \author{Samwise Gamgee}
% \affiliation{
%   \institution{Long Journey University}
%   \city{Eriador}
%   \country{New Zealand}}
% \email{samwise.gamgee@lju.nz}

% \author{Merry Brandybuck}
% \affiliation{
%   \institution{The Ring Company Inc.}
%   \city{Lindon}
%   \state{The Great Land}
%   \country{New Zealand}}
% \email{merry@ring.com}

% \author{Frodo Baggins}
% \affiliation{
%   \institution{Two Towers University}
%   \city{Rohan}
%   \country{New Zealand}}
% \email{frodo.baggins@2tu.nz}

%%% Use this environment to specify a short abstract for your paper.

\begin{abstract}

%Belief-Desire-Intention (BDI) is one of the most widely used models for developing software agents. 
Computational agents based on the BDI framework typically rely on abstract plans and plan refinement to reach a degree of autonomy in dynamic environments, and so they have an  %the 
ability %of an agent 
to select \textit{how-to} achieve their goals by choosing from a set of options. In this work we focus on a related, yet under-studied feature: % present in BDI frameworks, particularly in those based on the AgentSpeak(L) language:
%(although already present in frameworks as those based on the AgentSpeak(L) language)
\textit{abstract goals}. These constructs refer to the ability of agents to adopt goals that are not fully grounded at the moment of invocation, refining them only when and where needed, that is, the ability to select \textit{what-to} (concretely) achieve at run-time. We present a preference-based approach to goal refinement, and to do so, we propose a method to define preferences based on \textit{Ceteris Paribus} Networks (CP-Nets) for an AgentSpeak(L)-like agent programming language and utilize the well-defined CP-Net logic and algorithms to guide the goal refinement of the agent. As a technical contribution, we present an implementation of this method that solely uses a Prolog-like inference engine of the agent's belief-base to reason about preferences, thus %requiring minimal modification to the framework that does 
not affecting the decision-making mechanisms hard-coded in the agent framework. % behavior of the agent as when no preferences are defined.

\end{abstract}

%%% The code below was generated by the tool at http://dl.acm.org/ccs.cfm.
%%% Please replace this example with code appropriate for your own paper.


%%% Use this command to specify a few keywords describing your work.
%%% Keywords should be separated by commas.

\keywords{Agent-based Programming, BDI Agents, Abstract Goals, Preferences, CP-Nets}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Include any author-defined commands here.
         
\newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%% The following commands remove the headers in your paper. For final 
%%% papers, these will be inserted during the pagination process.

\pagestyle{fancy}
\fancyhead{}

%%% The next command prints the information defined in the preamble.

\maketitle 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

As computational agents intervene more and more in human activities, there is an increase demand for human-oriented views of programming, that is, relying on concepts and abstractions mapping intuitively to what humans utilize to explain and direct their behaviour. The \textit{belief-desire-intention} agent framework (BDI) \cite{Rao1995}, centred around a general theory of mind \cite{bratman1987intention}, offer one of those views, and has been studied by the community since the 90s, resulting in the proposal of several platforms for agent-based programming (e.g. Jason/AgentSpeak(L)).

Computational agents based on the BDI framework typically rely on abstract plans and plan refinement to reach a degree of autonomy in dynamic environments. In practice, this relative autonomy consists in the ability of an agent to select \textit{how-to} achieve their goals by choosing from a set of options. This contrasts with classic forms of planning, in which the agent receive a detailed policy, designed to reach a certain specific objective. BDI agent scripts typically consist of hierarchical, partial, abstract plans.  

In this work we focus on a related, yet under-studied feature: % present in BDI frameworks, particularly in those based on the AgentSpeak(L) language:
%(although already present in frameworks as those based on the AgentSpeak(L) language)
\textit{abstract goals}. These constructs refer to the ability of agents to adopt goals that are not fully grounded at the moment of invocation, refining them only when and where needed, that is, the ability to select \textit{what-to} (concretely) achieve at run-time. Usually associated to these goals are \textit{activity}-level characterizations of behaviour, e.g. walking for some time (where?), eating something (what?), meeting someone (who?), selling (what? to whom?), etc. 

The specification of abstract goals is a feature already present in some agent frameworks as those based on the AgentSpeak(L) language, albeit they rely on simplistic mechanisms of refinement. The present work aims to cover the passage from abstract goals to concrete goals in the agent's decision making cycle. 
We present a preference-based approach to goal refinement, and to do so, we propose a method to define preferences based on \textit{Ceteris Paribus} Networks (CP-Nets), utilizing the well-defined CP-Net logic and algorithms to guide the goal refinement of the agent, and targeting an AgentSpeak(L)-like agent programming language for implementation. Since the introduction of Jason,  AgentSpeak(L) programs are enriched with Prolog rules and facts to deal with knowledge-level processing, occurring e.g. for testing context conditions guiding the plan selection phase. As a technical contribution, we present therefore an implementation of a refinement method that solely uses a Prolog-like inference engine of the agent's belief-base to reason about preferences, thus %requiring minimal modification to the framework that does 
not affecting the decision-making mechanisms hard-coded in the agent framework, and so directly usable in current platforms. 

The paper proceeds as follows... 

\begin{comment}

[Copied]

In the last decades several attempts have been made to move from machine-oriented views of programming towards concepts and abstractions that more closely reflect the way in which humans conceive the world. In particular, the \textit{belief-desire-intention} framework (BDI) \cite{Rao1995}, building upon a theory of mind \cite{bratman1987intention}, has been introduced to provide a basis for the implementation of computational agents that exhibit rational behaviour, using the same representations that we typically use to address human behaviour. % Additionally, from a modeling  standpoint, the BDI framework offers a cognitive model for agent-based modelling (ABM) \cite{balke2014}, 
%\subsubsection{Why prioritize BDI scripts with preferences}
In the %related 
decision-making literature, instead, particular attention is given to the role of preferences: any model of agency involving decision-making is deemed to abide the agent's preferences \cite{Pigozzi2016}. This does not imply that any model of agency will rely on explicit preferences, rather it affirms the general principle that when there are multiple goals that should be achieved (or multiple ways to achieve a certain goal or even multiple sets of states that can be reached) the best course of action is the one that abides the most to the agent's preferences \cite{Pigozzi2016}.
In practice, preferences can vary from the implicit ``maximize utility" of optimizing agents \cite{Nunes2014} to explicit preferences specified in a preference representation language \cite{Dasgupta2010,Visser2011}. 
% BDI agent execution architectures, more often than not, rely on logical constructs for modeling the agents' mental components, see e.g. the critical overview given in \cite{Herzig2018}. 
Unexpectedly, none of the main BDI languages presented in the literature support explicit preferences. %Decisions between alternative choices (in our case, for plan selection) are generally based on implicit forms of preferences like sequential ordering (e.g. of plan specifications).% Only a few works have investigated a potential role for explicit preferences in BDI agents \cite{Dasgupta2010,Visser2011,Nunes2014,Mohajeri2019}

\end{comment} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Background} 
\label{sec:2}
[COPIED]
% This section provides an overview of the concepts and methods on which our contribution builds upon. Most of these sections are succinct version of what presented in \cite{Mohajeri2019}.
% \vspace{0pt} \noindent \textbf{BDI Agents} \quad
\subsection{BDI Agents}
Agents specified following a BDI framework are represented by three mental attitudes. Beliefs are facts that the agent believes to be true. Desires capture the motivational dimension of the agent, typically conflated with the more concrete form of \textit{goals}, representing procedures/states that the agent wants to perform/achieve. Intentions are selected conducts (or \textit{plans}) that the agent commits to (in order to advance its desires). 

Since their origin \cite{Rao1995}, the essential feature associated to BDI architectures is the ability to instantiate abstract plans that can (a) react to specific situations, and (b) be invoked based on their purpose. Consequently, the BDI execution model naturally relies on a \textit{reactive} model of computation, usually in the form of some type of \textit{event-condition-action} (ECA) rules often referred to as \textit{goal-plan rules}. Goal-plan rules are uninstantiated specifications of the \textit{means} (in terms of course of actions, or plan) for achieving a certain \textit{goal} \cite{Rao1995}. These constructs represent essentially the procedural knowledge (\textit{how-to}) of the agent. 

%\vspace{5pt} \noindent \textbf{Preferences in BDI agents} \quad
In current BDI implementations, preferences between these optional conducts are specified through a static ordering assigned by the programmer, typically via the ordering of rules in the code: the higher a rule is in the script, the more priority the associated plan has. This explains why most current frameworks including Jason \cite{Bordini2005}, 2APL \cite{Dastani2007}, ASC2 \cite{mohajeriparizi_2020_run}, etc. are genuinely \textit{reactive}: the scripts are interpreted without the need for any additional introspection/deliberation steps. However, these frameworks also expose \textit{plan selection} functions that can be modified to implement alternative mechanism for goal-plan rule selection during the deliberation cycle which can be considered as a type of meta-programming over agent scripts. % As it was said before, this approach has the positive features of simplicity, readability and performance while having the issue that these order-based preferences stay in the mind of programmer, and are not represented as explicit preferential knowledge.
% For more clarity, to better separate goal-adoption from the treatment of primitive actions, we will not consider primitive actions as part of preferences. 
%[???] 
%The latter option has been taken by almost all works adding explicit preferences to BDI agents \cite{Visser2011,Dasgupta2010,Nunes2014}: the selection of the most preferred alternative is taken as a \textit{reflective} process, where preferences provide a \textit{rationale} to be applied online during the agent's deliberation cycle.  The idea of relying on an offline step is instead proposed also in \cite{Mohajeri2019}, but they only focused on \textit{procedural preferences} (``I prefer to be doing $a_i$ rather than doing $a_j$"), which have a different level of abstraction w.r.t. to declarative preferences (``I prefer being in state $s_i$ rather than being in state $s_j$"). % This paper focused instead on \textit{declarative preferences}, i.e. about states of the world (``I prefer being in state $s_i$ rather than being in state $s_j$"). For this purpose, we used the notions of (1) \textit{declarative goals} (``want-to-be'' goals), and of (2) \textit{post-condition} or effect specification of \textit{primitive actions}. 
%Modifying the deliberation cycle by adding run-time reflective steps (as a complex preference checking and related plan selection algorithms) is generally detrimental to reactivity

%\vspace{5pt} \noindent \textbf{Preference languages} \quad

\subsection{Preference Languages}
Several models of preferences have been presented in the decision-making and planning literature, with various levels of granularity and expressiveness (see e.g. \cite{Domshlak2011}). The most straightforward \textit{quantitative} approaches are based upon \textit{utility theory} and related forms of decision theory. In \cite{Cranefield2017} one can find some examples of integration of these types of preferences in a BDI architecture. % A hybrid quantitative method is provided by PDDL3 \cite{Gerevini2005}, an extension of the \textit{planning domain definition language} (PDDL) \cite{McDermott1998}. Although based on qualitative descriptions, these preferences are considered quantitative \cite{Baier2007} because the valuation of each preference is expressed with a numerical value. 

Although quantitative approaches bring clear computational advantages, they also suffer from the non-trivial issue of translating users' preferences into utility functions. This explains the existence of a family of \textit{qualitative} or hybrid solutions, as LPP \cite{Bienvenu2006} and PDDL3 \cite{Gerevini2005}. Proposals exist for integrating LPP in BDI agents \cite{Visser2011}. Other preference models, as CP-nets (qualitative) \cite{Boutilier2004} and GAI networks (quantitative) \cite{Gonzales2004}, have been specifically introduced for taking into account dependencies and conditions between preferences via \textit{compact representations} \cite{Pigozzi2016}, highly relevant in domains with a large number of features. % Whereas CP-nets have weak constraints, GAI networks build upon the assumption of \textit{generalized additive independence}, and in doing so they enable computing the utility contribution of every single attribute/subset of attributes (they can be seen as the preferential counterpart of Bayesian networks). CP-nets and GAI-networks share possibility to be illustrated as intuitive graphical models. 
In the present work we will focus on CP-Nets, and their extention CP-Theories because they rely on weaker assumptions, and exhibit primarily a qualitative nature. To our knowledge, \cite{Mohajeri2019} was the first attempt to introduce this type of representational models in a BDI architecture, although focusing only on procedural preferences.
%This work continues the previous effort by considering declarative preferences.

More in detail, conditional \textit{ceteris paribus}  preferences networks (CP-nets) are a compact representation of preferences in domains with finite \textit{attributes of interest} \cite{Boutilier2004}. An attribute of interest is an attribute in the world (e.g. \textit{restaurant}) that the agent has some sort of preference over its possible values (e.g. \textit{italian} and \textit{french}). CP-nets build upon the idea that most of the preferences people make explicit are expressed jointly with an implicit \textit{ceteris paribus} (``all things being equal'') assumption. For instance, when someone says ``I prefer a french restaurant over an italian one'', they do not mean at all costs and situations, but that they prefer a french restaurant, all other things being equal. An example of conditional preference could be ``If I'm at a french restaurant, I prefer fish over meat''. CP-theories \cite{Wilson2004} add stronger conditional statements with a \textit{regardless of} part allowing some attributes to be released from the equality rule.

% As this work is an initial step in integrating preferences in BDI scripts the full power of CP-nets and its extensions is not utilized yet. % the point here is to show as a proof of concept how such compact presentation can be used in this manner.
%An attribute $A$ is said to be the parent of attribute $B$ if preferences over $B$ are conditional over values of $A$. 


\section{The Body of the Paper}

Highlight Points:
\begin{itemize}
    \item Explicit Preferences
    \item CP-nets
    \item preferability vs. applicability
    \item Comparing our CP-net with original
    \item Preferences are written in Logic form
    \item Reasoning only happens inside agent's belief base / minimal extra machinery
\end{itemize}

Need to be written somewhere:
\begin{itemize}
    \item AgentSpeak-like abstract goals
    \item We focus on achievement goals but the same principles apply to test and belief updates
    \item The interaction between plan selection and goal selectionp
\end{itemize}


\subsection{BDI Agents/Plan Selection}
Based on the definitions proposed by Rao et al. in \cite{Rao1995,RaoAS1996}, a BDI agent consists of a set of Beliefs $B$ referred to as belief base, a set of plans $P$ referred to as plan library ... 
\begin{definition}
[Relevant plan] A plan $p$, denoted by $e : C \Rightarrow H$ is a \textit{relevant plan} with respect to an event $\epsilon$ iff there exists a most general unifier $\sigma$ such that $\epsilon\sigma = e\sigma$. Then, $\sigma$ is referred to as the relevant unifier for $\epsilon$.
\end{definition}
\Gio{[here we should define what is meant by unifier]}
\begin{definition}
[Applicable plan] A plan $p$, denoted by $e : C \Rightarrow H$ is an \textit{applicable plan} with respect to an event $\epsilon$ iff there exists a relevant unifier $\sigma$ for $\epsilon$ and there exists a substitution $\delta$ such that $C\sigma\delta$
is a logical consequence of belief base $B$. The composition $\sigma\delta$ is referred to as the applicable unifier for $\epsilon$ and $\delta$ is referred to as a \textit{correct answer substitution}.
\end{definition}

\subsubsection*{Example 1}
Imagine an agent that upon request, can go to a restaurant and order a three-course meal, the script for such agent is presented in Listing \ref{lst:script_1}. The agent has two plans for going to a restaurant and ordering food, the first plans is \textit{applicable} if the agent not already at a restaurant which means an step of moving (\asc{#move_to} action) is needed prior to ordering the food, the second plan is applicable if the agent is already at the restaurant which means the it will just adopt the goal of ordering the food.

\begin{listing}[!h]
\centering
\begin{minted}[fontsize=\small]{prolog}
(P1) +!go_order(Loc,Meal) :
        restaurant(Loc) & not at(Loc) =>
        #move_to(Loc);
        !order(Meal).

(P2) +!go_order(Loc,Meal) :
        restaurant(Loc) & at(Loc) => 
        !order(Meal). 

(P3) +!order(meal(S,M,W)) : 
        soup(S) & main(M) & wine(W) => 
        #ask_waiter(meal(S,M,W)).
\end{minted}
    \caption{Script for Food-ordering Agent}
    \label{lst:script_1}
\end{listing}

\noindent Suppose the agent selects an event with the trigger: 
\begin{minted}[fontsize=\small]{prolog}
!go_order(french,meal(veg,meat,white))
\end{minted}
For this event two plans P1 and P2 are relevant with unifier $\sigma$:
\begin{minted}[fontsize=\small]{prolog}
{Loc/french, Meal/meal(veg,meat,white)}
\end{minted}
Assuming that the belief base of the agent contains the beliefs \asc{restaurant(french)} (meaning that there exists a French restaurant) and \asc{at(home)} (meaning that the agent is at home), then only the first plan will be an applicable plan for this event and the applicable unifier will be the same as the relevant unifier. This entails that only plan (P1) will be instantiated as: 
\begin{minted}[fontsize=\small]{prolog}
+!go_order(french,meal(veg,meat,white)) :
    restaurant(french) & not at(french) =>
    #move_to(french);
    !order(meal(veg,meat,white)).
\end{minted}


\subsection{Abstract Events/Goals}
Partial autonomy in dynamic environments is considered a core attribute of BDI agents, and this is in fact one of the reasons that separates plan refinement in BDI agents from classical planning approaches \cite{DeSilva2004}. While the idea of choosing between distinct plans to achieve a certain goal---typically referred to as plan selection---has been investigated by the community as the principal point of autonomous choice in BDI agents, there is indeed another important type of autonomy embedded in BDI agents: abstract events. While the previous example only exhibited fully grounded events, BDI agents and in particular those defined by \cite{Rao1995} and by extension available in the definition of AgentSpeak(L) \cite{RaoAS1996}, can indeed handle abstract events, referring to situations where an event contains unbounded variables and these variable can be grounded by different means such as context conditions of plans or test goals at any level in the plan refinement of the event. It can be argued that if plan selection promotes autonomy in the \textit{how-to-do} dimension of the agent, abstract events promote autonomy in selecting (concretely) \textit{what-to-do} with it.

\subsubsection*{Example 2}
Consider the same agent presented in listing \ref{lst:script_1}. This time we assume the agent has more information about the environment: it has beliefs about two types of soups, two types of main course, two types of wine, two restaurants, also it believes that it is already at one of the restaurants (french) and finally it has an inferential rule that says for all types of soup, main course and wine, there is a meal combination. Those beliefs can be presented as in listing~\ref{lst:beliefs_1}.

\begin{listing}[!tbh]
\centering
\begin{minted}[fontsize=\small]{prolog}
main(fish). main(meat).
soup(veg). soup(fish).
wine(white). wine(red).
restaurant(french). restaurant(italian).
at(french).
meal(S,M,W) :- soup(S), main(M), wine(W).
\end{minted}
    \caption{The beliefs of the food ordering agent}
    \label{lst:beliefs_1}
\end{listing}


Now assume the agent receives an abstract event \asc{!go_order(L,M)} which basically puts no constraints over where the agent should go and what it should order, and so gives it full autonomy to choose how to proceed. When the agent receives this event, both plans P1 and P2 are considered relevant plans with unifier \asc{{Loc/L, Meal/M}}. But considering the context conditions and the belief base, P1 will be applicable with unifier \asc{{Loc/italian, Meal/M}} and P2 with \asc{{Loc/french, Meal/M}} (note that in both cases the second parameter is not grounded and is only unified to another variable). At this point the agent's reasoning engine needs to use its plan selection function to choose one of the two plans. In both cases, the next event for the agent will be \asc{!order(M)} and P3 is a relevant plan for this event with unifier \asc{{M/meal(S,M,W)}}.  After taking into account the context condition, this event will have 8 different applicable unifiers with all possible combinations for the meal, e.g: \asc{{M/meal(veg,fish,white)}}, for which, again, the plan selection function needs to choose an option to start the actual execution. The goal-plan tree of this abstract goal can be seen in Figure~\ref{fig:gp-tree}.

\begin{figure}[!tbh]
  \centering
  \includegraphics[width=0.7\linewidth]{outfile.png}
  \caption{Plan/Event Refinement of the Agent}
  \label{fig:gp-tree}
  \Description{Possible Plan/Event Refinements of the Food-ordering Agent}
\end{figure}

\subsection{Preferences/CP-nets}

%Recalling on the CP-Theories \cite{Wilson2004}, assume a set of variables $X \in V$ each having a set finite set of values $x$, deonted $Dom(X)$, called the \textit{domain} of $X$. A value $u$ for $U \subseteq V$ associates with every $X \in U$ a value of $X$. The set of all values $u$ for $U$ is denoted $Dom(U)$, called the domain of $U$ and $\top$ the only value of $U=\emptyset$. A conditional preference $\lambda$ on $V$ is denoted by $u:x \succ x'[W]$, where $U \subseteq V$, $u \in Dom(U)$, $X \in V - U$, $x,x' \in Dom(X)$, and $W \subseteq V - (U \cup \{X\})$. This preference relation means given $t \in Dom(T)$ where $T=V-(U\cup\{X\}\cup W)$, then $x$ is preferred to $x'$, irrespective of the value of $W$. CP-Nets \cite{Boutilier2004} can be presented with $W=\emptyset$.

%Based on this definitions, an outcome $o \in Dom(V)$
\Mos{CP-nets and CP-Theories got very much intertwined here, need to clear up}

Recalling on the CP-Nets as defined in \cite{Boutilier2004}, assuming a set of variables $X \in V$ each having a set finite set of values $x$, involve conditional preference statements of the form $u : x \succ x'$, where $x$,$x'$ are assignments of a variable $X \in V$ and $u$ is an assignment to a set of variables $U \subseteq V$ (parents of $X$). The interpretation of this statement is that given $u$, then $x$ is preferred to $x'$ all else equal, meaning, for all assignment $s$ of the set of variables $S$, where $S = V - (U \cup \{X\})$, $sux$ is preferred to $sux'$, where $sux$ and $sux'$ are two \textit{outcomes} (complete assignment) to all variables of $V$. 

Building upon CP-Nets, in \cite{Wilson2004} the CP-Theories are introduced which add \textit{stronger conditional statements} to CP-Nets. These include preferential statements in the form $u : x \succ x' [W]$, where $W \subseteq V$ which interprets that for all assignments $w,w'$ to variables of $W$ and assignments to $t$ to variables of $T = V - (U \cup {X} \cup W)$, then the outcome $tuxw$ is preferred to the outcome $tux'w'$. This means that given $u$ and any $t$, then $x$ is preferred to $x'$ \textit{regardless} of assignments to $W$.

%It is shown that with both CP-Nets and CP-Theories, a set of such preference statements $\Lambda$ generates a partial ordering between all the outcomes of $V$, if $\Lambda$ is consistent, where consistency corresponds to the preference relations being acyclic with respect to the parent child relations of the variables. 
Assuming $\Lambda$ is a set of acyclic (with respect to parent-child relations) preference relations over variables of $V$, and considering $o,o'$ are outcomes of $V$, then $\Lambda$ is satisfiable iff there exists a preference ranking $\succ$ over the outcomes of $V$ that each $o \succ o'$ satisfies each of the preference statements of $\Lambda$. It is said that $\Lambda \models o \succ o'$ iff $o \succ o'$ holds in every preference ordering that satisfies $\Lambda$. Then $o$ and $o'$ can have one of the possible relations according to $\Lambda$: either $\Lambda \models o \succ o'$; or $\Lambda \models o' \succ o$; or $\Lambda \not\models o \succ o'$ and $\Lambda \not\models o \succ o'$. The third case means there is not enough information to prove either outcome is preferred. With these definitions in \cite{Boutilier2004}, two distinct ways for comparing outcomes are proposed:

\begin{itemize}
    \item Dominance queries: Asking if $\Lambda \models o \succ o'$ holds, which is referred to as $o$ is preferred to and \textit{dominates} $o'$.
    \item Ordering queries: Asking if $\Lambda \not\models o' \succ o$ holds, which is referred to as $o$ is preferred to $o'$.
\end{itemize}

Although ordering queries are weaker than dominance queries, they are still sufficient in many applications and will be used in this work. Furthermore, if an outcome $o$ is present such that there is no $o'$ such that $\Lambda \models o' \succ o$, then we say $o$ is \textit{undominated} or \textit{most preferred} with respect to $\Lambda$.
%One of the main usages of such partial ordering is comparing outcomes, The are two distinct ways proposed by \cite{Boutilier2004}


%While the idea of integrating CP-Nets (and CP-Theories) with BDI programs have been suggested before \cite{Mohajeri2019,Mohajeri2020}, there are important distinctions between the logic and algorithms defined for CP-Nets and those of BDI agents or even more broadly logic programs that need to be addressed. Firstly, by definition, CP-Nets rely on a closed world assumption, which means they assume a single static set of variables $V$ (or \textit{features} or \textit{attributes}) as the decision domain and all of the variables appearing in a conditional preference statement (or rule) should be part of $V$. Secondly, they focus attention on single-stage decision problems with complete information, ignoring any issues that arise in multi-stage, sequential decision analysis and any considerations of risk that arise in the context of uncertainty \cite{Boutilier2004}.% which means preference rules create a hierarchy between these variables.


%Contrary to CP-Nets, a BDI agent deals with many related or independent sets of variables in their decisions and can not be limited to one set. Also, a dynamic agent's factual, inferential, procedural or even preferential knowledge about the environment can be subject to revisions in the its life-cycle and it can not be assumed that all the variables are fully known at any point. Furthermore, BDI agents are designed to act in dynamic environments with incomplete information and uncertainty, and more often than not, a BDI agent deals with these issues via multi-stage decision making and dynamic reactions to the environment such as incremental plan and goal refinement and even failure handling.

To map the CP-Theories to BDI (Prolog-like) logics one should look at what needs to be decided in the process of goal refinement. An agent may have dynamically interconnected beliefs about the environment, but when it is deciding on what is the most preferred approach to partially ground the variables of an event or goal in the form of $!p(t_1,...,t_n)$, only the variables of that goal are part of the decision. Theoretically, in this approach we do not have only one CP-Theory, but each distinct goal/event has zero or more inferred CP-Theories from the set of all preference statements. 

In this work, the preferences of an agent are presented in a different notation from that of CP-Nets and CP-Theories, but more similar to OCP-Theories in \cite{DiNoia2015}.

A conditional preference statement $\lambda$ of the agent can be expressed in the form of inferential rules such as:
\begin{equation*}
    P \succ P' \leftarrow C
\end{equation*}
where $P,P'$ are predicates in the form $p(v_1,...,v_n)$ and $p(v'_1,...,v'_n)$,  $p$ can be any functor including those used as head of plans, each $v_i$ and $v'_i$ can be either a (partially) ground term, a named variable or an anonymous variable (underscore) and $C$ is an arbitrary logical expression that \textit{activates} the preference statement if it can be proven to be true at the time of evaluation, which can include variables that appear on the left side of the $\leftarrow$. The set of all preferences of an agent is referred to as $\Lambda$.

With this definition, for a predicate $P$ with the form $p(v_1,...,v_n)$, the set of all variables is defined as $V = \{v_1,...v_n\}$. The simplest form of preference statements that can be expressed with this form is that of CP-Theories. To express a conditional preference statement $u : x \succ x' [W]$ in this form, on a predicate $P$, assuming $P_X \in V$ is the variable of $P$ corresponding to $X$, the set $P_U \subseteq V$ is the set of all variables corresponding to $U$, the set $P_W \subseteq V$ is the set of all variables corresponding to $W$ and $P_T = V - (P_U \cup \{P_X\} \cup P_W)$, the statement can be presented as $P \succ P' \leftarrow true$, such that $P_X$ is written as $x$, $P'_X$ is written as $x'$, all the variables of $P_U$ and $P'_U$ are written as their corresponding value in $u$, all variables of $P_W$ and $P'_W$ are written as anonymous variables (underscore) and all the variables of $P_T$ and $P'_T$ are replaced with named variables that have the same name in both $P$ and $P'$.


E.g, the statement of $p(x,u,T,\_) \succ p(x',u,T,\_) \leftarrow true$ relays the preference that between two (partially) grounded terms with the functor $p$, if the second parameter can be unified to $u$ in both, and the third parameter is the equal, then the term that its first parameter can be grounded to $x$ is preferred to the one that its first variable can be ground to $x'$, regardless of the values of the fourth variables. 

CP-Theory statements can be expressed in the form of $P \succ P' \leftarrow C$ where $C=true$, but, as BDI agents are designed to act in dynamic environments with incomplete information and uncertainty, and more often than not, a BDI agent has to react with changes in the environment and failures; simply using static CP-Theory statements is not sufficient for a BDI agent. This is addressed by the \textit{activation} condition $C$ of preference statements. This condition can be any arbitrary Prolog-like query over both belief base of the agent and the variables appearing on the left side of the $\leftarrow$. At any point in the life-cycle of agent with a belief base $B$ and set of preferences $\Lambda$, a preference statement $\lambda = P \succ P' \leftarrow C$ is considered to be part of $\Lambda$ iff $C$ is a logical consequence of the $B$. This can drastically increase the expressivity of the preference statements in dynamic environments. The next example further explores these type of preferences.


%To extend the definition of $\succ$ to unifiers of a partially unbound term: given a database of facts and rule $B$ and a set of preferences $\Lambda$, if $\epsilon$ is a partially unbound term and $\delta,\delta'$ are two unifiers for $\epsilon$ such that $\epsilon\delta$ and $\epsilon\delta'$ are logical consequences of $B$, then if $\epsilon\delta \succ \epsilon\delta$


%Secondly, CP-Nets focus attention on single-stage decision problems with complete information, ignoring any issues that arise in multi-stage, sequential decision analysis and any considerations of risk that arise in the context of uncertainty \cite{Boutilier2004}. These limitations are mainly because CP-Nets require every possible value of each variable to be known prior to decision-making and also, a CP-Net needs to decide a value for each variable in the process of decision-making. These issues, are a detrimental contrast from BDI agents that are designed to act in dynamic environments with incomplete information and uncertainty, and more often than not, a BDI agent deals with these issues via multi-stage decision making and dynamic reactions to the environment such as incremental plan and goal refinement and even failure handling.
%  Furthermore, as the main point of this paper, a BDI agent with partially grounded goals, does not need to decide on every variable of a variable set in their decision and some variables may stay unbounded.
%  unlike many other works on integrating preferences into BDI agents \cite{Visser2011,Visser2016,Mohajeri2019,Mohajeri2019Prefs}, this work does not assume that all the possible values for variables are known at any point in agent's life-cycle

%The third core difference between CP-Nets and BDI agents, or more precisely agents that utilize Prolog-like reasoning concerns the algorithms used for finding decisions. The variables of a 

%Prior to addressing these issues, first a formal definition of a conditional preference relation of an agent is presented, which informally describes a rule that in the condition $C$ the term $T$ dominates the term $T'$. 




%\begin{definition}
%A conditional preference relationdenoted as $T \succ T' \leftarrow C$  given a database of facts and rules $B$, at any point in time, iff $C$ is a logical consequence of $B$.
%\end{definition}

\subsubsection{Example 3}
Consider some preferences over the actions of our food ordering agent, first we'll define some preferences over the meal: (R1) As the main course, meat is preferred to fish if at an Italian restaurant and (R2) fish is preferred to meat if at a French restaurant (R3) if the main course is meat, then a fish soup is preferred to vegetable soup and (R4) if the main course is fish, then a vegetable soup is preferred, (R5) for drinks, red wine is preferred to \textit{any} other type of drink if vegetable soup is in the meal and likewise, (R6) white wine is preferred to \textit{any} other drinks if fish soup is in the meal. 

%Note that R1 and R2, are conditional based on external factors

%In these preferences only the first one is unconditional and the other four are conditional, also note that for example in the second preference relation, while an explicit condition is put on the meat course or variable \asc{M}, there is also an implicit condition on the wine or \asc{W}, which dictates that in order for this preference relation to be applicable between two meals, the wine should be the same, this represents the \textit{all-else-equal} principle of the CP-Nets. 

While multiple preferences are defined over the meal, still they do not translate directly to any of the events that the agent can handle, to fix this, we add a simple but important (meta-)preference: (R7) in the event of ordering \textit{anything}, ordering something more preferred is also preferred to ordering something less preferred.% While this preference connection may seem obvious, defining such arbitrary preference relations can prove to be a powerful tool for the designer in many cases.

Next to define preferences about the restaurant: (R8) if the agent is already located at a restaurant, it is preferred to order at that restaurant (to not relocate) compared to {any} other restaurant regardless of the meal (R9) the combination of Italian restaurant + meat main dish is preferred to any other restaurant + main dish combination if the agent is not already at another restaurant. The preferences of the agent can be seen in listing \ref{lst:prefs_1}.

%Note that preference relation R1, R2, R8 and R9 are different from the rest because they are conditional based on the belief base of the agent and will be active if their condition is a logical consequence of the belief base $B$ at the time of plan selection. 

\begin{listing}[!tbh]
\centering
\begin{minted}[fontsize=\footnotesize]{prolog}
(R1) meal(S,meat,W) >> meal(S,fish,W) :- at(italian).
(R2) meal(S,fish,W) >> meal(S,meat,W) :- at(french).
(R3) meal(fish,meat,W) >> meal(veg,meat,W) :- true.
(R4) meal(veg,fish,W) >> meal(fish,fish,W) :- true.
(R5) meal(veg,M,red) >> meal(veg,M,_) :- true.
(R6) meal(fish,M,white) >> meal(fish,M,_) :- true.
(R7) !order(M1)  >> !order(M2) :- M1 >> M2.
(R8) !go_order(L,_) >> !go_order(_,_) :- at(L).
(R9) !go_order(italian,meal(S,meat,W)) >> !go_eat(L,meal(S,_,W)) 
    :-  not at(L).
\end{minted}
    \caption{The preferences of food ordering agent}
    \label{lst:prefs_1}
\end{listing}

As an example, the preferential relation graph associated to the predicate \asc{meal/3} with respect to preferences in listing \ref{lst:prefs_1} and the beliefs in listing \ref{lst:beliefs_1} is presented in figure~\ref{fig:cp-graph} which suggests that \asc{meal(veg,fish,red)} is the most preferred option. Among these preference statements only R3 and R4 are simple CP-Theory preferences. The statements R1, R2 are conditioned on beliefs that are external with respect to the variables of the preference itself: the location of the agent. This type of conditional statements result in multiple preferential relations that can be contradictory. In fact, the preference structure in figure~\ref{fig:cp-graph} would have been different if the agent had the belief \asc{at(italian)} instead of \asc{at(french)} which intuitively means the agent has different preferences about meals in different restaurants. Although, this only works because it is assumed that at most one of the two conditions \asc{at(french),at(italian)} can be true at any time and in fact, if both are true at any time then the two preferences are contradictory and the preference relations of \asc{meal/3} will be unsatisfiable.
%Furthermore, as other statements over \asc{meal/3} are dependant on (are child nodes of) the main dish, there are two different preferential structures associated to this predicate as shown in figure~\ref{fig:cp-graph}. 
The statement R5 and R6 specify preferences that although simple, can not be expressed in CP-Theories, they give conditional preference to a value of a variable (drinks) over any other value of that variable; this can be useful in cases where the domain of values of a variable are unknown at design time, but the designer is aware of a few values that are always either desired or should be avoided. 



As it was pointed out, R7 is a meta-preference, the condition of this statement does not consult the belief-base of the agent, but rather the preferences of the agent. This is more explained in section [...]. R8 not only creates a condition over the belief-base (\asc{at(L)}), but also connects the variable \asc{L} that will be grounded at run-time to the preference relation itself, creating an interesting preference of ``\textit{I prefer the place I am already at}''. Finally R9 also presents an statement that can not be expressed in CP-Theories, in which two different variables are part of the preference. This can be useful tool but also can easily lead to cyclic preference, thus, this type of preference statement should be used with care.

\begin{figure}[!tbh]
  \centering
  \includegraphics[width=0.7\linewidth]{cp-graph.png}
  \caption{Preference Structure of the Agent over \asc{meal/3}}
  \label{fig:cp-graph}
  \Description{Preference Structure of the Agent}
\end{figure}

\subsection{Goal Refinement via Preferences}
Integration of the preferences into the BDI reasoning cycle can be implemented as a \textit{unifier ordering} step prior the \textit{plan selection}, that only applies when the triggering event contains unbounded variables. To achieve this, when the agent selects a partially unbounded event $\epsilon$, first as part of the normal BDI reasoning cycle, it needs to find all of the relevant unifiers by consulting the plan library, and then find all of the applicable unifiers by consulting the belief base. Afterwards, the agent can create a partial ordering between the applicable unifiers. To create this ordering, the ordering queries defined in the previous section are extended to unifiers: given a set of preferences $\Lambda$, if $\epsilon$ is a partially unbound event and $(\sigma\delta),(\sigma\delta)'$ are two applicable unifiers for $\epsilon$, then $(\sigma\delta)$ is preferred to $(\sigma\delta)'$ iff $\Lambda \not\models (\sigma\delta)' \succ (\sigma\delta)$.


%In this ordering, a an applicable unifier $(\sigma\delta)$ precedes another applicable unifier $(\sigma\delta')$ for $\epsilon$, if $\epsilon(\sigma\delta)$ is not dominated by $\epsilon(\sigma\delta)'$ according to the preferences. Note that this will result in a partial ordering and some unifiers may be incomparable depending on the preferences. 

Finally the normal plan selection can continue w.r.t. the re-ordered list of unifiers, but now, plan selection function can take into account the ordering between unifiers. For simplicity, through the rest of the paper it is assumed the agent uses a typical plan selection function in BDI frameworks that selects the first applicable unifier for each event. This assumption alongside the ordering step that was presented means the agent always uses an \textit{undominated} or \textit{most preferred} unifier to ground the variables of a (partially) abstract goal. Thus, instead of the ordering, throughout the paper, the concept of most preferred unifier is used. 
%Note that the event does not need to be fully grounded and it may be that the unifier $\sigma$ keeps some of the variables unbounded or even introduce new variables. As this unifier 

\begin{definition}
A unifier $(\sigma\delta)$ is referred to as the and \textit{undominated} or \textit{most preferred} unifier for an event $\epsilon$ iff $\sigma\delta$ is an applicable unifier for $\epsilon$ and there is no other applicable unifier $(\sigma\delta)'$ for $\epsilon$, such that $\Lambda \models \epsilon(\sigma\delta)' \succ \epsilon(\sigma\delta)$ can be proven.
\end{definition}


\subsubsection{Example 4}
Consider again the agent in example 1, with beliefs in example 2 and this time with preferences in example 3, assume that this agent receives an abstract event \asc{!go_order(L,M)}, then like example 2 two applicable unifier will be created: P1 will be applicable with the unifier \asc{{Loc/italian, Meal/M}} and P2 with the unifier \asc{{Loc/french, Meal/M}}, because the agent has the belief \asc{at(french)} the preference relation R8 becomes active and the agent concludes that:
\begin{minted}[fontsize=\small]{prolog}
!go_order(french,M) >> !go_order(italian,M)
\end{minted}
Thus, the \asc{{Loc/french, Meal/M}} becomes the most preferred unifier and as only P2 is applicable with this unifier it will be selected. Next, when the sub-goal \asc{!order(Meal)} is being considered, there are 8 applicable unifiers for plan P3, and assuming the \asc{at(french)} belief still stands and based on the preference rules the ordering in fig [...] will be true, then, the most preferred unifier will be \asc{{S/veg, M/fish, W/red}}, meaning the goal will be refined to \asc{!order(meal(veg,fish,red))} which the plan for this goal (P3) will be instantiated by the plan selection.

\subsubsection{Example 5}
To show how partially abstract goals would be grounded with method, consider the same agent as before with the same set of preferences and beliefs, except this time the agent has the belief \asc{at(home)} instead of \asc{at(french)}, and the agent receives an event with ordering a meal with meal as the main course: \asc{!go_order(L,meal(S,meat,W))}. This time plan P2 with will be applicable with two unifiers:
\begin{minted}[fontsize=\small]{prolog}
{Loc/italian, Meal/meal(S,meat,W)}
{Loc/french, Meal/meal(S,meat,W)}
\end{minted}
and based on the some the preference rule R9, the Italian restaurant is preferred to any other restaurant as long as there is meat main course, so the first unifier will be the most preferred one and then the first plan is selected with it. Next, and assuming the \asc{move_to} action works correctly the belief \asc{at(home)} would be retracted, \asc{at(italian)} should be in the belief-base and then the sub-goal \asc{!order(meal(S,meat,W))} will be adopted. At this point, based on the preference rules the ordering in fig [...] will be true, and the most preferred unifier will be \asc{{S/fish, M/meat, W/white}} meaning the goal will be refined to \asc{!order(meal(fish,meat,white))} which the plan for this goal (P3) will be instantiated by the plan selection.

\Mos{NEED TO WRITE ABOUT THE TEMPORAL ASPECT OF THIS EXAMPLE HERE OR LATER ON, IN NORMAL CP-NETS THIS IS A CYCLE, FOOD DEPENDED ON LOCATION AND LOCATION DEPENDED ON FOOD, BUT HERE BECAUSE THERE ARE DIFFERENT CHOICE POINTS ITS NOT A PROBLEM}

\section{Implementation}
With the transformation \textit{CP-Net} logic to \textit{Prolog} logic proposed in section [], a practical implementation of the method is introduced...



Next, as the method relies only on the belief-base of the agent, a supplementary predicate called $applicable/1$ is introduced where for each plan $e : C \Rightarrow H$, an inference rule in the form of 
\begin{equation}
applicable(e) \leftarrow C
\end{equation}
is added to the belief base of the agent at compile time. Intuitively, querying this predicate with an event $\epsilon$ as the argument will return all possible applicable groundings of that event that can be concluded from the plan library and the belief base. As an example, querying the term \asc{applicable(go_eat(L,M))} on the agent in the examples will result in two answers: \asc{go_eat(italian,M)} and \asc{go_eat(french,M)}.

\begin{listing}[!h]
\centering
\begin{minted}[fontsize=\footnotesize]{prolog}
%
:- dynamic(at/1).
:- dynamic(meal/3).
:- dynamic(order/1).
:- dynamic(go/1).
:- dynamic(go_eat/2).


%PREFERENCES
dominates(P,P):- !,false.
dominates(meal(S,main(meat),W),meal(S,main(fish),W)) :- at(italian).
dominates(meal(S,main(fish),W),meal(S,main(meat),W)) :- at(french).
dominates(meal(soup(fish),M,W),meal(soup(veg),M,W)) :- M == main(meat).
dominates(meal(soup(veg),M,W),meal(soup(fish),M,W)) :- M == main(fish).
dominates(meal(S,M,wine(red)),meal(S,M,_)) :- S == soup(veg).
dominates(meal(S,M,wine(white)),meal(S,M,_)) :- S == soup(fish).
dominates(go_eat(L1,_),go_eat(_,_)) :- at(L1).
dominates(go_eat(italian,meal(S,main(meat),W)),go_eat(L,meal(S,_,W))) :- \+at(L).

%APPLICABILITY
meal(soup(S),main(M),wine(W)) :- soup(S), main(M), wine(W).
go_eat(L,M) :- location(L), \+at(L).
go_eat(L,M) :- location(L),at(L).


%The ONLY RULE!
most_prefered(Pred) :-
    copy_term(Pred,Pred2), Pred, forall(Pred2,(dominates(Pred2,Pred)->fail;true)).

%BELIEFS
main(fish).
main(meat).
soup(veg).
soup(fish).
wine(white).
wine(red).

at(italian).

location(italian).
location(french).

\end{minted}
    \caption{The preferences of food ordering agent}
    \label{lst:prefs_1}
\end{listing}


\subsection{Complexity}
\begin{acks}
If you wish to include any acknowledgments in your paper (e.g., to 
people or funding agencies), please do so using the `\texttt{acks}' 
environment. Note that the text of your acknowledgments will be omitted
if you compile your document with the `\texttt{anonymous}' option.
\end{acks}

\section*{General thoughts (to see whether it makes sense to add somewhere)}

We can define at least three categories of preferences related to BDI reactive agents. The first category is that of \textit{volitional preferences}, which essentially maps to desires ($D_{\mathsf{pref}}$, ``I want this more than that''). The second category is about \textit{epistemic preferences}, which maps to beliefs ($B_{\mathsf{pref}}$, ``I believe this more than that''). Abstract goals do not fully specify their objectives; yet, their satisfaction requires their objective to be made concrete at some point, some form of progressive unveiling, in order to be reached into the world. For instance, if I want to talk with someone, first I need to meet someone somewhere, and in order to meet someone in some place, I need to believe that there is some person there. The mere knowledge that there is someone in some place it is however not sufficient to start the action. The agent needs to be practically able to reach that place. 

Abilities are a special type of beliefs, which do not concern more generally the world, but what the agent can reach via a certain behaviour (or plan, in the sense of course of action). Abilities can be preferred in terms of \textit{effectiveness} (e.g. how often they reach certain outcomes) and/or \textit{efficacy} (i.e. to what extent their effects satisfy volitional preferences). Whereas effectiveness is measured against certain goals (possibly abstract), efficacy requires an evaluation taking into account a richer preferential structure. To make this elaboration more explicit, given a certain plan $p$, we can define a number of bindings $(p, o)$ for each possible effect $o$ such plan may produce. The strength of bindings can be  e.g. measured empirically, resulting in a triple $(p, o, w)$, where $w$ captures the strength. [Additionally, these results may be observable only in given conditions, which for simplicity we do not consider here.] So, given a certain plan $p$, we can construct two vectors consisting of all outcomes $\vec{o}$ obtained by performing $p$, with a relative strength $\vec{w}$, resulting in the data structure $(p, \vec{o}, \vec{w})$. Given a target $o$, we can define  $A^*_{\mathsf{pref}}(o)$, a preferential structure across plans using their associated $w$. Given a certain plan $p$ however, we can also consider an aggregated evaluation of $\vec{o}$ by $D_\mathsf{pref}$ discounted by $
\vec{w}$ to provide a measure of general efficacy of that conduct. In this case the preferential ordering does not depend on the target outcome $c$: $A_{\mathsf{pref}}^{**}$.

In principle, the agent has an intended outcome $o$, therefore it focuses on effectiveness first, via $A^*_{\mathsf{pref}}(o)$, purging the space of possible solutions. On a second instance, however, the agent can check the efficacy of the most effective plans via a portion of $A_{\mathsf{pref}}^{**}$. As it often happens, the most effective plans could be also very expensive/costly, so a conflict will emerge in the two preferential structure that requires an external input. If conflicts are all solved by the designer, one can construct a general preferential structure $A$ standing for the \textit{practical preferences} of the agents.

Starting from an abstract goal, the concretization requires certain facts, and thus, from an agent perspective, the existence of some beliefs. If those are not available/formed yet, the agent may look for evidence for them. This search in itself has to be associated to an ability, and needs to pass through a similar analysis as the above. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% The next two lines define, first, the bibliography style to be 
%%% applied, and, second, the bibliography file to be used.

\bibliographystyle{ACM-Reference-Format} 
\bibliography{sample}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

